# -*- coding: utf-8 -*-
"""VoiceCloning.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ybjipo0JzMucI5AdUJZ4xIkasjo4Pwcr
"""

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

import librosa
import numpy as np
import librosa.display
import soundfile as sf

# Load audio file
audio, sr = librosa.load("/content/drive/MyDrive/Deep learning project/sample.wav", sr=22050)

# Trim silence from audio
audio_trimmed, _ = librosa.effects.trim(audio)

# Normalize audio
audio_normalized = librosa.util.normalize(audio_trimmed)

# Save the cleaned audio
sf.write("/content/drive/MyDrive/Deep learning project/processed_voice.wav", audio_normalized, sr)

import torch
import torch.nn as nn
import torch.optim as optim

class TextToMel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(TextToMel, self).__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.attention = nn.Linear(hidden_dim, hidden_dim)
        self.decoder = nn.LSTM(hidden_dim, output_dim, batch_first=True)

    def forward(self, x):
        encoded, _ = self.encoder(x)
        attention_weights = torch.softmax(self.attention(encoded), dim=-1)
        context = torch.bmm(attention_weights.unsqueeze(1), encoded)
        mel_output, _ = self.decoder(context)
        return mel_output

# Hyperparameters
input_dim = 256  # Example input dimension for phonemes
hidden_dim = 512
output_dim = 80  # Number of mel-spectrogram bins

# Initialize model, loss, and optimizer
model = TextToMel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()  # Loss function for mel-spectrogram
optimizer = optim.Adam(model.parameters(), lr=0.001)

class Vocoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Vocoder, self).__init__()
        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=5)
        self.upsample = nn.Upsample(scale_factor=4, mode="linear")
        self.conv2 = nn.Conv1d(hidden_dim, 1, kernel_size=5)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.upsample(x)
        waveform = self.conv2(x)
        return waveform

# Initialize vocoder
vocoder = Vocoder(input_dim=80, hidden_dim=256)
vocoder_optimizer = optim.Adam(vocoder.parameters(), lr=0.0001)
vocoder_loss = nn.MSELoss()  # Reconstruction loss

def text_to_speech(text, text_to_mel_model, vocoder_model):
    # Convert input text to phoneme encoding
    phonemes = text_to_phonemes(text)  # Custom function to encode text

    # Generate mel-spectrogram
    mel_spectrogram = text_to_mel_model(phonemes)

    # Generate audio waveform
    waveform = vocoder_model(mel_spectrogram)
    return waveform

import streamlit as st

st.title("Synthetic Voice Cloning")
text_input = st.text_input("Enter text to synthesize:", "Hello, how are you?")

if st.button("Generate Voice"):
    # Call the text_to_speech function
    waveform = text_to_speech(text_input, model, vocoder)
    st.audio(waveform.detach().numpy(), format="audio/wav")